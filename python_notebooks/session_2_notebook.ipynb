{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages imported\n"
     ]
    }
   ],
   "source": [
    "# torch library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distrib \n",
    "\n",
    "# for example dataset \n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for plotting \n",
    "import matplotlib.pyplot as plt \n",
    "import sys, os \n",
    "from pathlib import Path\n",
    "import json \n",
    "\n",
    "print('Packages imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all type hints you might need \n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading MNIST\n"
     ]
    }
   ],
   "source": [
    "# load a couple of data sets \n",
    "\n",
    "main_path = Path(os.getcwd())\n",
    "\n",
    "# --- MNIST\n",
    "dataset_path = main_path.joinpath('mnist')\n",
    "batch_size = 100 \n",
    "\n",
    "def get_transforms(with_normalize=False) -> Callable:\n",
    "    if not with_normalize:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "    else:\n",
    "       return transforms.Compose([\n",
    "           transforms.ToTensor(),\n",
    "           transforms.Normalize((0.1307,), (0.3081,))]) \n",
    "\n",
    "mnist_trans = get_transforms(True)\n",
    "\n",
    "mnist_train_dataset = MNIST(dataset_path, transform=mnist_trans, train=True, download=True)\n",
    "mnist_test_dataset = MNIST(dataset_path, transform=mnist_trans, train=False, download=True)\n",
    "print('Done loading MNIST')\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network definition is done.\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Basic neural networks\n",
    "    - feed forward networks \n",
    "    - convolutional networks\n",
    "    - recurrent networks (RNN, LSTMS, ... )\n",
    "    - transformers\n",
    "'''\n",
    "\n",
    "import abc \n",
    "\n",
    "# --- Basic neural networks --- \n",
    "class BaseNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
    "        super(BaseNeuralNet, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # (feature learning) hidden layers (stacking of multiple of hidden units + nonlinearity(per neuron) + batch_normalization + [dropouts] + [layer normalization])\n",
    "        self.hidden_layers = None \n",
    "        self.fl = None\n",
    "        \n",
    "        self._init_hidden_layers()\n",
    "        self._final_layer()\n",
    "\n",
    "        # predict probablity\n",
    "        self.prob_dist = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        \n",
    "        assert self.hidden_layers is not None, 'Hidden layers are not initialized'\n",
    "        assert self.fl is not None, 'Final layer is not initialized'\n",
    "\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _init_hidden_layers(self): ...\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _transform_input(self, x): ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _final_layer(self): ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._transform_input(x)\n",
    "        x = self.hidden_layers(x) # this is the forward pass\n",
    "        probs = self.prob_dist(self.fl(x))   # predicts the probablity as a multinomial distribution\n",
    "        return probs\n",
    "\n",
    "\n",
    "# --- Feed forward neural networks ---\n",
    "class FeedForwardNeuralNet(BaseNeuralNet):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
    "        super(FeedForwardNeuralNet, self).__init__(input_dim, hidden_dim, output_dim, activation)\n",
    "\n",
    "    def _init_hidden_layers(self):\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim, bias=False),               # what does bias do?\n",
    "            self.activation,\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim), \n",
    "            self.activation,\n",
    "            nn.BatchNorm1d(self.hidden_dim),                          # what does batch normalization do?\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            self.activation,\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "        )\n",
    "\n",
    "    def _final_layer(self):\n",
    "        # final hidden layer (linear classifier)\n",
    "        self.fl = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def _transform_input(self, x):\n",
    "        return x.flatten(start_dim=1)\n",
    "\n",
    "# --- Convolutional neural networks ---\n",
    "class ConvolutionalNeuralNet(BaseNeuralNet):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
    "        super(ConvolutionalNeuralNet, self).__init__(input_dim, hidden_dim, output_dim, activation)\n",
    "\n",
    "    def _init_hidden_layers(self):\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, self.hidden_dim, kernel_size=3, stride=1, padding=1),   # what is hidden_dim, kernel_size, stride, padding?\n",
    "            self.activation,\n",
    "            nn.BatchNorm2d(self.hidden_dim),\n",
    "            nn.Conv2d(self.hidden_dim, self.hidden_dim, kernel_size=3, stride=1, padding=1),\n",
    "            self.activation,\n",
    "            nn.BatchNorm2d(self.hidden_dim),\n",
    "        )\n",
    "\n",
    "    def _final_layer(self):\n",
    "        # final hidden layer (linear classifier)\n",
    "        self.fl = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(self.hidden_dim * 28 * 28, self.output_dim)\n",
    "            )\n",
    "\n",
    "    def _transform_input(self, x):\n",
    "        return x\n",
    "\n",
    "# --- Recurrent neural networks ---\n",
    "# class RecurrentNeuralNet(BaseNeuralNet):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
    "#         super(RecurrentNeuralNet, self).__init__(input_dim, hidden_dim, output_dim, activation)\n",
    "\n",
    "#     def _init_hidden_layers(self):\n",
    "#         return nn.LSTM(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "#     def _transform_input(self, x):\n",
    "#         return x\n",
    "\n",
    "print('Network definition is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ Training config ++\n",
      "{\n",
      "  \"optimizer\": {\n",
      "    \"optimizer_name\": \"SGD\",\n",
      "    \"learning_rate\": 0.001\n",
      "  },\n",
      "  \"loss_fn\": \"NLLLoss\",\n",
      "  \"device\": \"cuda\",\n",
      "  \"model\": {\n",
      "    \"model name\": \"ConvolutionalNeuralNet\",\n",
      "    \"activation\": \"ReLU\"\n",
      "  },\n",
      "  \"train_details\": {\n",
      "    \"epoch\": 10,\n",
      "    \"batch_size\": 50\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Start model training ... \n",
      "Epoch 0: Train loss: 0.2194210561364889\n",
      "--------------------\n",
      "Epoch 0: correct classification rate : 96.95%\n",
      "--------------------\n",
      "Epoch 1: Train loss: 0.09441337041168785\n",
      "Epoch 2: Train loss: 0.06986510061154452\n",
      "Epoch 3: Train loss: 0.05714324823968733\n",
      "Epoch 4: Train loss: 0.048433887737919575\n",
      "Epoch 5: Train loss: 0.042193252614621694\n",
      "--------------------\n",
      "Epoch 5: correct classification rate : 98.38%\n",
      "--------------------\n",
      "Epoch 6: Train loss: 0.037580068613751794\n",
      "Epoch 7: Train loss: 0.033634164133303175\n",
      "Epoch 8: Train loss: 0.030309426025875535\n",
      "Epoch 9: Train loss: 0.02756496482451136\n",
      "Epoch 10: Train loss: 0.02507606964771791\n",
      "--------------------\n",
      "Epoch 10: correct classification rate : 98.48%\n",
      "--------------------\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "# How to train and eval a neural network\n",
    "\n",
    "class NetworkTrainer:\n",
    "    def __init__(self, optimizer, loss_fn, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "    def train_neural_net(self, model, train_loader, test_loader, epochs=10) -> None:\n",
    "        print('Start model training ... ') \n",
    "        for epoch in range(epochs + 1):\n",
    "            train_loss = self._train_step(model, train_loader)\n",
    "            print(f'Epoch {epoch}: Train loss: {train_loss}')\n",
    "            if epoch % 5 == 0:\n",
    "                #test_loss = self.eval_neural_net(model, test_loader)\n",
    "                accuracy = self._accuracy(model, test_loader)\n",
    "                print('--------------------')\n",
    "                print(f'Epoch {epoch}: correct classification rate : {accuracy * 100.}%')\n",
    "                print('--------------------')\n",
    "            \n",
    "\n",
    "    def _train_step(self, model, train_loader):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            self.optimizer.zero_grad()  # removes any \n",
    "            output = model(data)\n",
    "            loss = self.loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # average loss over all batches \n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "    def eval_neural_net(self, model, test_loader):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = model(data)\n",
    "                total_loss += self.loss_fn(output, target).item()\n",
    "        # \n",
    "        return total_loss / len(test_loader)\n",
    "\n",
    "    def _accuracy(self, model, data_loader) -> torch.Tensor:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in data_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        # \n",
    "        # FIXME: decompose this into precision, recall, balanced accuracy, f1-score, ...\n",
    "        return correct / total\n",
    "\n",
    "\n",
    "# --- Training a neural network ---\n",
    "# check if cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# select different datasets. \n",
    "train_dataset = mnist_train_dataset\n",
    "test_dataset = mnist_test_dataset\n",
    "\n",
    "# initalize data loaders\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# initalize the model\n",
    "input_dim = 784         # input dimension\n",
    "hidden_dim = 100        # feature dimension\n",
    "output_dim = 10         # number of classification class\n",
    "activation = nn.ReLU()  # activation function\n",
    "\n",
    "#-- select model \n",
    "# feed-forward neural network\n",
    "model = FeedForwardNeuralNet(input_dim, hidden_dim, output_dim, nn.ReLU()).to(device)\n",
    "\n",
    "# convolutional neural network\n",
    "# input_dim = 1\n",
    "# hidden_dim = 32\n",
    "# model = ConvolutionalNeuralNet(input_dim, hidden_dim, output_dim, nn.ReLU()).to(device)\n",
    "\n",
    "\n",
    "# select optimizer and loss functions \n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "\n",
    "# training config \n",
    "train_config = {\n",
    "    'optimizer':{'optimizer_name': optimizer.__class__.__name__, 'learning_rate': learning_rate},\n",
    "    'loss_fn': loss_fn.__class__.__name__,\n",
    "    'device': device.type,\n",
    "    'model' : {'model name': model.__class__.__name__, 'activation': activation.__class__.__name__},\n",
    "    'train_details':{'epoch': epochs, 'batch_size': batch_size}\n",
    "}\n",
    "\n",
    "print('++ Training config ++')\n",
    "print(json.dumps(train_config, indent=2))\n",
    "print('\\n')\n",
    "# initalize the training loop\n",
    "\n",
    "trainer = NetworkTrainer(optimizer, loss_fn, device)\n",
    "trainer.train_neural_net(model, train_loader, test_loader, epochs=epochs)\n",
    "print('Done training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common good to keep in mind concepts introduced in neural networks.\n",
    "# - activation functions\n",
    "# - loss functions\n",
    "# - optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative model + autoencoders. (VAE - variational auto encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applied ml (metric learning) --> select few examples. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
